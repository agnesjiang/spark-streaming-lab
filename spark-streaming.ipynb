{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Streaming Lab\n",
    "\n",
    "In this lab, you will explore the functionality of Spark Streaming and how it's used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will load a set of JSON files that comes from the  Heterogeneity Human Activity Recognition Dataset. The data consists of smartphone and smartwatch sensor readings from a variety of devicesâ€”specifically, the accelerometer and gyroscope, sampled at the highest possible frequency supported by the devices. Readings from these sensors were recorded while users performed activities like biking, sitting, standing, walking, and so on. \n",
    "\n",
    "The path to the files is `s3://bigdatateaching/spark-streaming-sample-data/`. In the following cell, create a static DataFrame object called `static` that reads the JSON files and uses the JSON read function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = [[fill this partk]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a streaming version of the same Dataset, which will read each input file in the dataset one by one as if it was a stream. Lookup the `spark.readStream.schema` function and use the appropriate parameters. Fill in the missing parts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = [[fill this part]]\\\n",
    "    .option(\"maxFilesPerTrigger\", 1)\\\n",
    "    .json([[fill this part]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, you will perform one simple transformation. You will group and count data by the gt column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = [[fill this part]].groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've setup your transformation, you need to specify the your action to start the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n",
    "  .format(\"memory\").outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how many streams are active at a given point in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see streaming in action, the next cell you will run a SQL query on the `activity_counts` that was previously generated. This will run the SQL query 5 times, waiting one second, and you'll see how the data changes as more data is read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql([[fill in this part]]).show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "  .where(\"stairs\")\\\n",
    "  .where(\"gt is not null\")\\\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"simple_transform\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "  .drop(\"avg(Arrival_time)\")\\\n",
    "  .drop(\"avg(Creation_Time)\")\\\n",
    "  .drop(\"avg(Index)\")\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()\n",
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    "  .cube(\"gt\", \"model\").avg()\\\n",
    "  .join(historicalAgg, [\"gt\", \"model\"])\\\n",
    "  .writeStream.queryName(\"device_counts_2\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from device_counts_2\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
